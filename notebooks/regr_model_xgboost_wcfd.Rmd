---
title: "Regression Feature Selection Wrapper: XGBoost w/ BCOGC data"
author: "Scott McKean"
output:
  pdf_document: default
  html_document: default
---

# Geoscience BC Study Code - Machine Learning Wrapper

```{r setup, include=FALSE}
library(geosciencebc2019008)
run_date = Sys.Date() %>% str_replace_all(., "-", "_")

model_prefix = 'xgboost_wcfd'
```

## ML Dataframe Prep

This chunk prepares our dataframe for clustering, statistical modelling, and other data science techniques. It specifies auxillary columns (those useful for information), predictor columns, and our target column(s): seimogenic (T/F) and max magnitude (numeric).

For the regression problem, we focus exclusively on wells that have generated earthquakes. The dataset is therefore quite a bit smaller, but non-seismogenic wells provide no information regarding the maximum magnitude of earthquake generated by an event.

```{r}
target = "max_mag"

aux_cols <- c(
  "unique_surv_id", "wa_num", "drilling_event", "ground_elevtn",
  "mean_easting", "mean_northing", "on_prod_date",
  "last_reported_date", "cum_gas_to_date_e3m3", "cum_oil_to_date_m3",
  "cum_water_to_date_m3", "cum_cond_to_date_m3", "frac_start_date",
  "frac_end_date", "min_dist_well", "calc_completed_length_m")
```

## Load and subset data

```{r}
# load prepared data
ml_df <- read_rds('../wcfd_data/wcfd_mldf.rds') %>%
  dplyr::filter(seismogenic == 1) %>%
  dplyr::select(-'seismogenic', -aux_cols)
```

## Feature Selection

```{r}
# normalize features to speed model fitting
norm_ml_df <- normalizeFeatures(ml_df, target = target)

# task
regr_tsk = makeRegrTask(data = norm_ml_df, target = target)

# learner
regr_lrn = makeLearner("regr.xgboost")
```

## filter-based feature importance

```{r}
plot_regr_filt_importance(regr_tsk,
paste0('../output/',model_prefix,"_"))
```

```{r, eval = FALSE}
## wrapper-based feature importance - sequential forward sampling
for (alpha in seq(5E-4, 9.9E-4, length.out = 50)){
  i = which(alpha == seq(5E-4, 9.9E-4, length.out = 50))
  
  regr_feats_seq = selectFeatures(
  learner = regr_lrn, task = regr_tsk, resampling = cv5,
  control = makeFeatSelControlSequential(method = 'sfbs', alpha = alpha), 
  measures = list(mae, rmse), show.info = FALSE
  )

  print(i)
  saveRDS(
    regr_feats_seq,
    paste0('../output/regr_feats_',model_prefix,'/seq_',i,'.rds')
    )
}

```

```{r}
regr_feats_seq_files = list.files(
  paste0('../output/regr_feats_',model_prefix), 
  pattern = "seq", full.names = TRUE
  )

for (file in regr_feats_seq_files){
  feat_res = read_rds(file)
  
  if (file == regr_feats_seq_files[1]){
    path = feat_res$opt.path$env$path
  } else {
    path = rbind(path, feat_res$opt.path$env$path)
  }
}

plot_regr_feat_seq(path, paste0('../output/',model_prefix),
                   n_best_val = 1000)

```

```{r}
## wrapper-based feature importance - random sampling
for (i in 1:100){
  regr_feats_rand = selectFeatures(
    learner = regr_lrn, task = regr_tsk, resampling = cv5,
    control = makeFeatSelControlRandom(maxit = 1000), 
    measures = list(mae, rmse), show.info = FALSE
    )
  
  print(i)
  saveRDS(regr_feats_rand, paste0('../output/regr_feats_rand_',i,'.rds'))
}
```

```{r}
regr_feats_rand_files = list.files(
  "../output/", 
  pattern = "regr_feats_rand_",
  full.names = TRUE
  )

for (file in regr_feats_rand_files){
  feat_res = read_rds(file)
  
  if (file == regr_feats_rand_files[1]){
    path = feat_res$opt.path$env$path
  } else {
    path = rbind(path, feat_res$opt.path$env$path)
  }
}

plot_regr_feat_rand(path, '../output/', n_best_val = 1000)

```

## Model Tuning

```{r}
final_feats = c(
  "distance_thrust_faults_berger_m", "top_montney_tvd_mss", 
  "top_montney_isotherm_degc", "mean_intervals_per_stage", 
  "horiz_wells_in_10km", "mean_isip_mpa",
  "well_completed_length_m", "mean_stage_duration_min"
  )
  
final_ml_df <- norm_ml_df %>% dplyr::select(final_feats, target)

final_regr_tsk = makeRegrTask(data = final_ml_df, target = target)

```

```{r}
xgb_ps = makeParamSet(
  makeNumericParam("eta", lower = 0.1, upper = 0.6),
  makeNumericParam("gamma", lower = 0.1, upper = 10),
  makeNumericParam("lambda", lower = -1, upper = 2, trafo = function(x) 10^x),
  makeIntegerParam("nrounds", lower = 50, upper = 150),
  makeIntegerParam("max_depth", lower = 1, upper = 6),
  makeNumericParam("colsample_bytree", lower = 0.3, upper = 0.7),
  makeNumericParam("alpha", lower = 0, upper =1),
  makeIntegerParam("min_child_weight", lower = 1, upper = 7)
)

# tune model
regr_tuned_params = tuneParams(
  learner = regr_lrn, task = final_regr_tsk, resampling = cv5,
  par.set = xgb_ps, control = makeTuneControlMBO(budget = 25), 
  measures = list(mae,rmse)
  )

#save tune results
saveRDS(regr_tuned_params, 'regr_tune_res.rds')
```

```{r}
regr_tune_res = readRDS('regr_tune_res.rds')

# set hyperparameters
tuned_regr_lrn = setHyperPars(
  learner = regr_lrn, par.vals = regr_tune_res$x
  )

# train model
regr_mod <- train(tuned_regr_lrn, final_regr_tsk)

# predict
regr_predict <- predict(regr_mod, task = final_regr_tsk)

# make IML predictor
regr_predictor = Predictor$new(
  regr_mod, 
  data = final_ml_df
  )
```

## Residual Analysis

```{r}
plt = plotResiduals(regr_predict)

plt + 
  ggtitle("") +
  theme_minimal() +
  ggsave('../output/regr_residual_plot.jpg', width = 10, height = 6)

plotLearnerPrediction(tuned_regr_lrn, task = regr_tsk, 
                      features = c("distance_thrust_faults_berger_m", "well_completed_length_m"))
```

## Feature Importance and Interaction

```{r}
# importance plot
regr_importance = getFeatureImportance(regr_mod)
feat_imp = plot_feat_imp(regr_importance, output_path = '../output/',
                          label = 'regr')

p1 = ggplot(feat_imp$data) +
  geom_col(aes(x = key, y = value)) +
  labs(x = 'Feature', y = 'Importance') +
  coord_flip() +
  theme_minimal()

interact = Interaction$new(regr_predictor)

p2 = ggplot(as.data.frame(interact$results)) +
  geom_col(aes(x = .feature, y = .interaction)) +
  labs(x = '', y = 'Interaction') +
  coord_flip() +
  theme_minimal()

library(gridExtra)

ggsave(file = '../output/regr_imp_interact.jpg', 
       arrangeGrob(grobs = list(p1,p2), nrow=1),
       width = 12, height = 8)
```

## Partial Dependence & ICE Plots

```{r}
# partial dependence plot
make_pdp_plot <- function(i, predictor, feats){
  pdp <- FeatureEffect$new(predictor, method = 'pdp+ice', 
                           feature = feats[i], center.at = 2)
  
  plot(pdp) + theme_minimal() + theme(axis.title.y=element_blank())
}

plist <- map(.x = 1:length(final_feats), .f = make_pdp_plot,
             predictor = regr_predictor, feats = final_feats)

ggsave(file = '../output/regr_pdp_plot.jpg', 
       arrangeGrob(grobs = plist, ncol = 3, left = 'Magnitude'),
       width = 10, height = 6)

```

## LIME

Global surrogate model can improve the understanding of the global model behaviour. We can also fit a model locally to understand an individual prediction better. The local model fitted by LocalModel is a linear regression model and the data points are weighted by how close they are to the data point for wich we want to explain the prediction.

An alternative for explaining individual predictions is a method from coalitional game theory named Shapley value. Assume that for one data point, the feature values play a game together, in which they get the prediction as a payout. The Shapley value tells us how to fairly distribute the payout among the feature values.

```{r}
good_wells = c(473, 480, 425, 426)
ml_df[good_wells,] %>% pull('max_mag')

row = 426

lime.explain = LocalModel$new(regr_predictor, 
                              x.interest = final_ml_df[row,],
                              k = length(final_feats))

p1 = ggplot(lime.explain$results) +
  geom_col(aes(x = feature, y = effect)) +
  labs(x = 'Feature', y = 'LIME Effect') +
  coord_flip() +
  theme_minimal()

shapley = Shapley$new(regr_predictor,
                      x.interest = final_ml_df[row,] %>% 
                        dplyr::select(final_feats),
                      sample.size = 100)

p2 = ggplot(shapley$results) +
  geom_col(aes(x = feature, y = phi)) +
  labs(x = 'Feature', y = 'SHAP Phi') +
  coord_flip() +
  theme_minimal()

ggsave(file = paste0('../output/regr_',row,'_lime_shap.jpg'),
       arrangeGrob(grobs = list(p1,p2), nrow=1),
       width = 12, height = 8)

```