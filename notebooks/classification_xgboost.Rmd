---
title: "Induced Seismcity Wrapper"
author: "Scott McKean"
output:
  pdf_document: default
  html_document: default
---

# Geoscience BC Study Code - Machine Learning Wrapper

```{r setup, include=FALSE}
library(geosciencebc2019008)
run_date = Sys.Date() %>% str_replace_all(., "-", "_")
knitr::opts_chunk$set(eval = FALSE)

# load prepared data
saf_wells_sf <- read_rds('../output/2020_06_27_saf_well_data.rds')
```

## ML Dataframe Prep

This chunk prepares our dataframe for clustering, statistical modelling, and other data science techniques. It specifies auxillary columns (those useful for information), predictor columns, and our target column(s): seimogenic (T/F) and max magnitude (numeric).

```{r}
aux_cols <- c(
  "unique_surv_id", "wa_num", "drilling_event", "ground_elevtn",
  "mean_ss_easting", "mean_ss_northing", "survey_well_type", 
  "on_prod_date", "last_reported_date", "cum_gas_to_date_e3m3", 
  "cum_oil_to_date_m3", "cum_water_to_date_m3", "cum_cond_to_date_m3",
  "frac_start_date", "frac_end_date", "calc_completed_length_m",
  "job_objective", "well_comments","min_dist_well")

removed_inputs <- c(
  "no_prior_wells", "no_surrounding_wells",
  "interval_clusters_bool", "base_fluid_cat",
  "hybrid_frac_bool", "skipped_stages", "stim_company")

numeric_input_cols <- c(
  "mean_ss_tvd", "n_stages", "well_completed_length_m",
  "mean_proppant_per_stage_t", "sd_proppant_per_stage_t", 
  "max_proppant_per_stage_t", "calc_total_proppant_t", 
  "total_gas_injected_m3", "mean_fluid_per_stage_m3", 
  "sd_fluid_per_stage_m3", "max_fluid_per_stage_m3", "calc_total_fluid_m3",
  "avg_stage_length", "avg_stage_spacing", "max_rate_m3_min", 
  "mean_rate_m3_min", "max_stage_duration_min", "mean_stage_duration_min",
  "max_breakdown_mpa", "sd_breakdown_mpa", "mean_breakdown_mpa", 
  "max_isip_mpa", "sd_isip_mpa", "mean_isip_mpa", "frac_duration_days", 
  "calc_proppant_intensity_kg_m", "calc_fluid_intensity_m3_m", "breakdown_isip_ratio", 
  "max_treating_mpa", "sd_treating_mpa", "mean_treating_mpa", 
  "number_sand_off_screen_out_stages", "mean_intervals_per_stage", "min_midpoint_dist",  
  "horiz_wells_in_1km", "horiz_wells_in_5km", "horiz_wells_in_10km",
  "horiz_wells_in_25km")

geology_input_cols <- c("distance_all_faults_berger_m",
"distance_divergent_faults_berger_m", "distance_listric_faults_berger_m", 
"distance_normal_faults_berger_m", "distance_strike_slip_faults_berger_m", 
"distance_thrust_faults_berger_m", "geothermal_gradient_degc_km", 
"paleozoic_structure_mss", "pressure_depth_ratio_kpa_m", "shmin_grasby", 
"third_order_residual_m", "top_montney_isotherm_degc", "top_montney_structure_mss", 
"top_montney_tvd_mss")

fact_input_cols <- c(
  "viscosity_cat", "energizer_bool", "proppant_cat",
  "cased_well_bool", "interference_bool")

target <- c("seismogenic")

ml_df <- saf_wells_sf %>%
  st_drop_geometry() %>% 
  dplyr::select(aux_cols, numeric_input_cols, fact_input_cols, geology_input_cols, target) %>%
  dplyr::mutate_if(is.logical, as.numeric) %>%
  dplyr::filter(
    max_stage_duration_min < 10000,
    max_treating_mpa < 200,
    max_breakdown_mpa < 200,
    min_midpoint_dist < 25000,
    max_isip_mpa < 200,
    shmin_grasby > 0) %>%
  dplyr::filter(
    !is.na(mean_proppant_per_stage_t),
    !is.na(mean_fluid_per_stage_m3),
    !is.na(mean_isip_mpa),
    !is.na(mean_breakdown_mpa),
    mean_ss_tvd < 0,
    min_midpoint_dist >0) %>%
  dplyr::mutate(seismogenic = as.factor(seismogenic)) 

# impute non-causal features
ml_df = impute(ml_df, cols = list(
  sd_proppant_per_stage_t = imputeMean(),
  sd_fluid_per_stage_m3 = imputeMean(),
  avg_stage_length = imputeMean(),
  avg_stage_spacing = imputeMean(),
  mean_rate_m3_min = imputeMean(),
  mean_stage_duration_min = imputeMean(),
  sd_treating_mpa = imputeMean(),
  sd_breakdown_mpa = imputeMean(),
  sd_isip_mpa = imputeMean(),
  mean_treating_mpa = imputeMean(),
  sd_treating_mpa = imputeMean(),
  mean_intervals_per_stage = imputeMean()
  ))$data

# one hot encode categorical variables
ml_df <- createDummyFeatures(
  ml_df, target = "seismogenic", cols = c("viscosity_cat","proppant_cat")
  ) %>% removeConstantFeatures(.)

ml_df[!is.finite(ml_df$max_rate_m3_min),'max_rate_m3_min'] <- 0
ml_df[!is.finite(ml_df$max_stage_duration_min),'max_stage_duration_min'] <- 0
ml_df[!is.finite(ml_df$max_treating_mpa),'max_treating_mpa'] <- 0

table(ml_df$seismogenic)/nrow(ml_df)

nrow(ml_df)
```

## Exploratory Data Analysis

```{r}
# scatter plot of variables with target
target_boxplot(ml_df, output_path = '../output/')

# corrplot of numeric features
ml_corrplot(ml_df, '../output/', numeric_input_cols, target)

# pca plot
pca_scree_plot(ml_df, '../output/')
pca_unit_circle_plot(ml_df, '../output/')
```

## Feature Selection/

- We normalize features to speed and improve model training

- We use the default xgboost parameters in the MLR package in order to provide a consistent
model for feature selection (eta = 0.3)

```{r}
# normalize features to speed model fitting
norm_ml_df <- normalizeFeatures(ml_df %>% select(-aux_cols), target = target)

# task
classif_tsk = makeClassifTask(data = norm_ml_df, target = target) %>%
  undersample(., rate = 0.25)

# learner
classif_lrn = makeLearner("classif.xgboost", predict.type = "prob")

## filter-based feature importance
plot_classif_filt_importance(classif_tsk, '../output/')
```

```{r, eval = FALSE}
## wrapper-based feature importance - sequential floating backwards sampling
for (alpha in seq(3E-3, 8E-3, length.out = 10)){
  i = which(alpha == seq(3E-3, 8E-3, length.out = 10)) + 10
  
  classif_feats_seq = selectFeatures(
  learner = classif_lrn, task = classif_tsk, resampling = cv5,
  control = makeFeatSelControlSequential(method = 'sfbs',alpha = alpha), 
  measures = list(logloss), show.info = FALSE
  )

  print(i)
  saveRDS(classif_feats_seq, paste0('../output/classif_feats_seq',i,'.rds'))
}

```

```{r, eval=False}
classif_feats_seq_files = list.files(
  "../output/", 
  pattern = "classif_feats_seq",
  full.names = TRUE
  )

for (file in classif_feats_seq_files){
  feat_res = read_rds(file)
  
  if (file == classif_feats_seq_files[1]){
    path = feat_res$opt.path$env$path
  } else {
    path = rbind(path, feat_res$opt.path$env$path)
  }
}

plot_classif_feat_seq(path, '../output/', n_best_val = 1000)
```

```{r, eval = False}
## wrapper-based feature importance - random sampling
for (i in 11:100){
  classif_feats_rand = selectFeatures(
    learner = classif_lrn, task = classif_tsk, resampling = cv5,
    control = makeFeatSelControlRandom(maxit = 1000), 
    measures = list(logloss), show.info = FALSE
    )
  
  print(i)
  saveRDS(classif_feats_rand, paste0('../output/classif_feats_rand_',i,'.rds'))
}
```

```{r, eval = False}
classif_feats_rand_files = list.files(
  "../output/", 
  pattern = "classif_feats_rand_",
  full.names = TRUE
  )

for (file in classif_feats_rand_files){
  feat_res = read_rds(file)
  
  if (file == classif_feats_rand_files[1]){
    path = feat_res$opt.path$env$path
  } else {
    path = rbind(path, feat_res$opt.path$env$path)
  }
}

plot_classif_feat_rand(path, '../output/', n_best_val = 1000)
```

## Final Model Tuning

```{r}
final_feats = c("n_stages", "well_completed_length_m", "min_midpoint_dist", "avg_stage_length",
  "calc_fluid_intensity_m3_m", "max_rate_m3_min", "distance_listric_faults_berger_m",
  "shmin_grasby","geothermal_gradient_degc_km")

final_ml_df <- norm_ml_df %>% dplyr::select(final_feats, target)

final_class_tsk = makeClassifTask(data = final_ml_df, target = target)
```

```{r}
xgb_ps = makeParamSet(
  makeNumericParam("eta", lower = 0.1, upper = 0.6),
  makeNumericParam("gamma", lower = 0.1, upper = 10),
  makeNumericParam("lambda", lower = -1, upper = 2, trafo = function(x) 10^x),
  makeIntegerParam("nrounds", lower = 50, upper = 150),
  makeIntegerParam("max_depth", lower = 1, upper = 6),
  makeNumericParam("colsample_bytree", lower = 0.3, upper = 0.7),
  makeNumericParam("alpha", lower = 0, upper =1),
  makeIntegerParam("min_child_weight", lower = 1, upper = 7)
)

# tune model
classif_tune_res = tuneParams(
  learner = classif_lrn, task = final_class_tsk, resampling = cv5,
  par.set = xgb_ps, control = makeTuneControlMBO(budget = 25L), 
  measures = logloss
  )

#save tune results
saveRDS(classif_tune_res, 'classif_tune_res.rds')
```

```{r}
classif_tune_res = readRDS('classif_tune_res.rds')

# set hyperparameters
tuned_classif_lrn = setHyperPars(
  learner = classif_lrn, par.vals = classif_tune_res$x
  )

# train model
classif_mod <- train(tuned_classif_lrn, final_class_tsk)

# predict
classif_predict <- predict(classif_mod, task = final_class_tsk)

# make IML predictor
class_predictor = Predictor$new(
  classif_mod, 
  data = final_ml_df
  )
```

## Residual / Confusion Matrix / Threshold

```{r}
# threshold vs performance
pvs = generateThreshVsPerfData(classif_predict, measures = list(fpr, tpr, mmce))
pvs_plot = plotThreshVsPerf(pvs)

pvs_plot +
  theme_minimal() +
  ggsave('../output/classif_perf_plot.jpg', width = 10, height = 6)

plotROCCurves(pvs)

```

## Feature Importance and Interaction

```{r}
# importance plot
classif_importance = getFeatureImportance(classif_mod)
feat_imp = plot_feat_imp(classif_importance)

p1 = ggplot(feat_imp$data) +
  geom_col(aes(x = key, y = value)) +
  labs(x = 'Feature', y = 'Importance') +
  coord_flip() +
  theme_minimal()

interact = Interaction$new(class_predictor)

p2 = ggplot(as.data.frame(interact$results) %>% filter(.class == 'X1')) +
  geom_col(aes(x = .feature, y = .interaction)) +
  labs(x = '', y = 'Interaction') +
  coord_flip() +
  theme_minimal()

library(gridExtra)

ggsave(file = '../output/classif_imp_interact.jpg', 
       arrangeGrob(grobs = list(p1,p2), nrow=1),
       width = 12, height = 8)
```

## Confusion Matrix and Threshold

```{r}
calculateConfusionMatrix(classif_predict, relative = TRUE, sums = TRUE)

plotLearnerPrediction(tuned_classif_lrn, task = classif_tsk, 
                      features = c("distance_listric_faults_berger_m", 
                                   "geothermal_gradient_degc_km")
                      )

```

## Partial Dependence & ICE Plots

```{r}
# partial dependence plot
make_pdp_plot <- function(i, predictor, feats){
  pdp <- FeatureEffect$new(predictor, method = 'pdp+ice', 
                           feature = feats[i], center.at = 0.5)
  
  plot(pdp) + theme_minimal() + theme(axis.title.y=element_blank())
}

plist <- map(.x = 1:length(final_feats), .f = make_pdp_plot,
             predictor = class_predictor, feats = final_feats)

ggsave(file = '../output/classif_pdp_plot.jpg', 
       arrangeGrob(grobs = plist, ncol = 3, left = 'Seismogenic'),
       width = 10, height = 6)

```

## LIME & SHAP

```{r}
good_wells = c(1823, 2042, 2049, 1930, 1931)
ml_df[good_wells,] %>% pull('seismogenic')

row = 1931

lime.explain = LocalModel$new(class_predictor, 
                              x.interest = final_ml_df[row,],
                              k = length(final_feats))

p1 = ggplot(lime.explain$results %>% filter(.class == 'X1')) +
  geom_col(aes(x = feature, y = effect)) +
  labs(x = 'Feature', y = 'LIME Effect') +
  coord_flip() +
  theme_minimal()

shapley = Shapley$new(class_predictor,
                      x.interest = final_ml_df[row,] %>% 
                        dplyr::select(final_feats),
                      sample.size = 100)

p2 = ggplot(shapley$results %>% filter(class == 'X1')) +
  geom_col(aes(x = feature, y = phi)) +
  labs(x = 'Feature', y = 'SHAP Phi') +
  coord_flip() +
  theme_minimal()

ggsave(file = paste0('../output/classif_',row,'_lime_shap.jpg'),
       arrangeGrob(grobs = list(p1,p2), nrow=1),
       width = 12, height = 8)

```